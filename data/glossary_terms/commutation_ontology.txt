COMMUTATION ONTOLOGY

Within Bohr’s conceptual framework, 
when the agential cut in the continuum of 
reality is enacted, that is the moment of a 
measurement could be described as clear 
but not distinct. Th is moment is clear 
insofar as rela tions can be drawn, space–
time–matter is made speciﬁ c, and an 
epistem o lo gical distinc tion between the 
object and the agency of the obser va tion is 
estab lished. However, the moment of the 
agen tial cut is not distinct insofar as it is 
onto lo gic ally insep ar able from the continu-
ity of the world. Th e causal struc ture of 
both models neither follows a ﬁ xed traject-
ory nor is completely left  to chance. In the 
quantum model, determ in ism is avoided 
through the mech an ism of the agen tial cut 
which intra- actively fore grounds certain 
exclu sions and there fore opens up a poten-
ti al ity that would be other wise closed-oﬀ  
in a deter m in istic sequen tial causal struc-
ture ( Barad 2007 : 179–82). Th e model does 
not entail a preformed matrix of possib il it-
ies that a pre- traced path will actu al ize. Th e 
non- arbit rar i ness of the model lies in the 
fact that new exclu sions and inclu sions are 
enacted in every intra- action, there fore 
recon ﬁ g ur ing any possib il it ies. 
 Following 
Barad’s 
‘ethico- onto-
epistemology’ ( Barad 2007 : 185), a non- 
sequen tial co- consti tu tion of cause and 
eﬀ ect simul tan eously emerges on each side 
of the agen tial cut. Humanist ethics, based 
on the human subject as the locus of 
respons ib il ity, require a repres ent a tion of 
the ‘other’. Th is condi tion of onto lo gical 
separ ab il ity is prob lem at ized by models 
based on entan gle ment (quantum physics) 
and the milieu (Deleuze’s model) as they 
co- consti tute cause and eﬀ ect, space, time 
and agency and they acknow ledge no prior 
separ ab il ity between the terms. Th e spati al ity 
of this ethical model makes the anthro po-
centric temporal causal struc tures redund-
ant and sets up a topo lo gical ground ing for 
new ethical models. In the same way that 
are co- consti tuted ‘genet ic ally . . . in the 
ensemble of the diﬀ er en tial rela tion 
in the subject’ ( Smith 2012 : 55). Th e calcu-
lus model has been gener ally criti cized 
for distin guish ing between the continu ity 
of space–time and the discrete ness of a 
matter that is pre- formed and given. In the 
above model, the present would proceed 
from the past in a determ in istic manner 
and would lead to the future within a 
smooth, continu ous fully deﬁ n able move-
ment. However, in Leibniz’s diﬀ er en tial 
calcu lus model, the object and subject as 
well as the time and space are all the result 
of the diﬀ er en tial rela tion: they are its 
products. Th e rela tion is external to the 
relata; in a way that it precedes and 
consti tutes them, deﬁ n ing what Deleuze 
would term as ‘diﬀ er ence- in-itself’ ( Smith 
2012 : 53). 
 In both the quantum and the diﬀ er en-
tial model, there are no pre- exist ent entit-
ies to be empir ic ally discovered by the 
subject but rather any temporal and spatial 
rela tions are contin gent on how the cut has 
been enacted on the rela tion ship of 
commut a tion. Th e basis for this commut a-
tion brings forward the insepar ab il ity of 
the objects and the ‘agen cies of obser va-
tions’ ( Barad 2007 : 114). 
 Th e result of this insepar ab il ity was 
observed by Niels Bohr as he could only 
measure the values of objects that parti cip-
ated in the exper i ment, as framed by the 
appar atus. His conclu sion that ‘obser va-
tion- inde pend ent objects do not possess 
well- deﬁ ned inher ent prop er ties’ ( Barad 
2007 : 196) ﬁ nds reson ances with the irre-
du cib il ity of the prin ciples of ‘clear and 
distinct’ developed by Descartes and refor-
mu lated by Leibniz. Leibniz envi sioned 
that ‘conscious percep tions [of the subject/
obser v ant] are neces sar ily clear but 
confused (not distinct), while uncon scious 
percep tions (Ideas) are distinct but neces-
sar ily obscure (not clear)’ ( Smith 2012 : 55).
the task of break ing down has been super-
seded by the func tion of elab or a tion for 
which auto mated machines learn from 
data by estab lish ing infer en tial rela tions 
between facial images and names, or voice 
frequency and patterns. Th e speed of 
correl a tion between inde pend ent chunks 
of data corres ponds to algorithmic func-
tions of elab or a tion for which this and that 
kind of inform a tion become infer en tially 
processed, elab or ated upon through the 
linear and non- linear logic of implic a-
tion for which a general rule or truth 
can be determ ined. With the devel op-
ment of arti ﬁ  cial neural networks and 
the method of machine learn ing, the turn 
to compu ta tion means the emer gence 
of auto mated systems of know ledge 
(see  AI ). 
 Th e idea of auto mated know ledge, 
however, is not new and needs to be histor-
ic ally mapped back to the very inven tion 
of the alpha betic atom iz a tion of language. 
With the discret iz a tion of the continu ity 
and local ity of speech, the alpha bet already 
deﬁ ned a compu ta tional order of commu ni-
c a tion that estab lished gram mat ical rules 
and a syntactical arrange ment of letters 
that could contain inﬁ n ite quant it ies of 
meaning. Despite the argu ment that com-
pu ta tional func tions are present in nature 
(i.e. the idea of a pan- compu ta tional 
universe), one cannot over look how the 
histor ical inven tion of formal language 
rather expressed the compu ta tional func-
tion of break ing know ledge into bits but 
also re- assem bling bits to trans mit, store 
and commu nic ate meaning. As long ago as 
the early 1700s, Gottfried Leibniz imagined 
a univer sal formal language able to express 
scientiﬁ c and meta phys ical concepts. His 
idea of  char ac ter ist ica univer salis consti-
tuted the basis of the alpha bet of human 
thought working as a discrete machine 
that could deliver ideas and things directly. 
an entan gle ment refuses prior distinc tions 
and pre- exist ing entit ies, a posthu man 
ethical model denies the split between the 
human and non- human. Instead, it rethinks 
the two terms as entan gle ments of each 
other with the bound ar ies that can be 
drawn between them as purely contin gent 
on the epistem o lo gical cut that is enacted. 
A commut a tion onto logy that recog nizes 
the insepar ab il ity of agency and the 
simul tan eity of cause and eﬀ ect can 
radic ally shift  human excep tion al ism but 
also raise the ethical stakes as agency 
becomes distrib uted and spatial, instead of 
pre- given and temporal. As agency lays 
new ground in this never the less ground-
less model, ethics becomes extremely 
contin gent to the mater ial eﬀ ects on 
the bodies that are consti tuted by the 
agen tial cut. 
 See also Locality/Non- separ ab il ity; Post-
human Ethics; Posthumanist Perform-
ativity; Ontological Turn; Multiverse. 
 Lila Athanasiadou 
 COMPUTATIONAL TURN  
 In the twenty- ﬁ rst century, the epistem o lo-
gical domin ance of comput ing has not 
only reduced know ledge to inform a tion 
but inform a tion itself has come to coin cide 
with large chunks of highly complex data 
that learn ing algorithms correl ate and 
continu ously model. Th is new form of 
discret iz a tion involves not only a break ing 
down of inform a tion chunks into bits that 
can be more easily reas sembled and clas si-
ﬁ ed. Instead, in this century the compu ta-
tional turn has been demarc ated by a new 
kind of inform a tion processing able to not 
only divide and add strings of data accord-
ing to programmed func tions. Th e devel-
op ment of learn ing algorithms in arti ﬁ  cial